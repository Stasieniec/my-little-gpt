import math
import os
import sys
import time
import warnings
from pathlib import Path

# Add project root to python path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


import fire
import numpy as np
import torch

from models import ByteTransformer
from data.enwik8_dataset import get_loader


def eval_model(
    checkpoint_path,
    
    # Evaluation parameters
    batch_size=64,
    num_workers=4,
    device="cuda" if torch.cuda.is_available() else "cpu",
    
    # Sampling parameters
    sample=True,
    sample_len=1024,
    temperature=0.8,
    num_samples=1,
    seed_text="\n",
):
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if "config" in checkpoint:
        config = checkpoint["config"]
    else:
        config = checkpoint
    
    # Model with the same configuration
    model = ByteTransformer(
        vocab_size=config.get("vocab_size", 256),
        block_size=config.get("block_size", 256),
        n_layer=config.get("n_layer", 8),
        n_head=config.get("n_head", 8),
        n_embd=config.get("n_embd", 512),
        dropout=config.get("dropout", 0.1),
    ).to(device)
    
    # Load weights
    if "model" in checkpoint:
        # strict=False to ignore unexpected keys in state_dict
        model.load_state_dict(checkpoint["model"], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    # Evaluate on test set
    block_size = config.get("block_size", 256)
    test_loader = get_loader(
        split="test",
        block_size=block_size,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=(device == "cuda"),
    )
    
    model.eval()
    test_losses = []
    
    with torch.no_grad():
        for batch in test_loader:
            inputs = batch["input"].to(device)
            targets = batch["target"].to(device)
            
            logits, loss = model(inputs, targets)
            test_losses.append(loss.item() / math.log(2))  # Convert to BPB
    
    avg_test_loss = np.mean(test_losses)
    print(f"Test set bits-per-byte: {avg_test_loss:.4f}")
    
    # Generate samples
    if sample:
        
        for sample_idx in range(num_samples):
            print(f"\nSample {sample_idx+1}:")
            
            # Convert seed text to byte indices
            seed_bytes = np.array(list(seed_text.encode('utf-8', errors='ignore')), dtype=np.int64)
            seed_tensor = torch.tensor(seed_bytes, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension
            
            # Generate
            start_time = time.time()
            
            # Generate tokens and convert back to text
            generated = model.generate(
                idx=seed_tensor, 
                max_new_tokens=sample_len,
                temperature=temperature,
            )
            
            # Convert generated bytes back to UTF-8 text
            generated_bytes = generated[0].cpu().numpy()  # Remove batch dimension
            generated_text = bytes(generated_bytes).decode('utf-8', errors='ignore')
            
            generation_time = time.time() - start_time
            tokens_per_sec = sample_len / generation_time
            
            print(f"Generated {sample_len} tokens in {generation_time:.2f}s ({tokens_per_sec:.2f} tokens/s)")
            print("-" * 40)
            print(generated_text)
            print("-" * 40)
    
    return avg_test_loss


if __name__ == "__main__":
    fire.Fire(eval_model) 