import os
import sys
import time
import warnings
from pathlib import Path

# Add project root to python path for imports
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))


import fire
import numpy as np
import torch

from models import ByteTransformer

def generate_text(
    # Model checkpoint
    checkpoint_path,
    
    # Generation parameters
    prompt="\n",
    max_tokens=1024,
    temperature=0.8,
    top_k=0,
    top_p=0.9
    
    output_file='generated_text',
    
    device="cuda" if torch.cuda.is_available() else "cpu"
):

    checkpoint_path = Path(checkpoint_path)
    
    # Check if checkpoint_path is a directory
    if checkpoint_path.is_dir():
        # Find the best or final model
        best_model_path = checkpoint_path / "best_model.pt"
        final_model_path = checkpoint_path / "final_model.pt"
        
        if best_model_path.exists():
            checkpoint_path = best_model_path
            print(f"Using best model: {checkpoint_path}")
        elif final_model_path.exists():
            checkpoint_path = final_model_path
            print(f"Using final model: {checkpoint_path}")
        else:
            # Any .pt file
            pt_files = list(checkpoint_path.glob("*.pt"))
            if pt_files:
                checkpoint_path = pt_files[0]
                print(f"Using checkpoint: {checkpoint_path}")
            else:
                raise FileNotFoundError(f"No checkpoint found in {checkpoint_path}")
            
        checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # Different checkpoint formats
    if "config" in checkpoint:
        config = checkpoint["config"]
    else:
        config = checkpoint
    
    # Model with the same configuration
    model = ByteTransformer(
        vocab_size=config.get("vocab_size", 256),
        block_size=config.get("block_size", 256),
        n_layer=config.get("n_layer", 8),
        n_head=config.get("n_head", 8),
        n_embd=config.get("n_embd", 512),
        dropout=config.get("dropout", 0.1),
    ).to(device)
    
    # Load weights
    if "model" in checkpoint:
        # strict=False to ignore unexpected keys in state_dict
        model.load_state_dict(checkpoint["model"], strict=False)
    else:
        model.load_state_dict(checkpoint, strict=False)
    
    model.eval()
    print(f"Model loaded successfully with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} parameters")
    
    # Convert seed text to byte indices
    seed_bytes = np.array(list(prompt.encode('utf-8', errors='ignore')), dtype=np.int64)
    seed_tensor = torch.tensor(seed_bytes, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension
    
    # Generate
    start_time = time.time()
    
    # generate function from ByteTransformer
    if top_k == 0 and top_p == 0:
        generated = model.generate(
            idx=seed_tensor, 
            max_new_tokens=max_tokens,
            temperature=temperature,
        )
    
    # Convert generated bytes back to UTF-8 text (ignore errors)
    generated_bytes = generated[0].cpu().numpy()  # Remove batch dimension
    generated_text = bytes(generated_bytes).decode('utf-8', errors='ignore')
    
    generation_time = time.time() - start_time
    tokens_per_sec = max_tokens / generation_time
    
    print(f"Generated {max_tokens} tokens in {generation_time:.2f}s ({tokens_per_sec:.2f} tokens/s)")
    print("-" * 40)
    print(generated_text)
    print("-" * 40)
    
    # Save to file
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(generated_text)
        print(f"Output saved to {output_file}")
    
    return generated_text

if __name__ == "__main__":
    fire.Fire(generate_text) 